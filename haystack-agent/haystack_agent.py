import os
import argparse
from haystack import Pipeline
from haystack.dataclasses import ChatMessage
from haystack.components.builders import ChatPromptBuilder
from haystack.components.generators.chat import OpenAIChatGenerator
from haystack_experimental.components.tools.openapi import OpenAPITool, LLMProvider

# Ensure required environment variables are set
if "OPENAI_API_KEY" not in os.environ:
    raise ValueError("OPENAI_API_KEY environment variable not set")

# Constants
ALLOW_DANGEROUS_REQUEST = True

# Initialize the OpenAPI tool with the F1 API schema
f1_tool = OpenAPITool(
    generator_api=LLMProvider.OPENAI,
    spec="openapi.yaml",
)

# Define the prompt
messages = [
    ChatMessage.from_system("Answer the F1 query using the API. Race names with two words should be separated by an underscore and be in lowercase. The API stores data from 2021 to 2024."),
    ChatMessage.from_user("User asked: {{user_message}}"),
    ChatMessage.from_system("API responded: {{service_response}}")
]

builder = ChatPromptBuilder(template=messages)

# Initialize the LLM
llm = OpenAIChatGenerator(generation_kwargs={"max_tokens": 1024})

# Define the pipeline components
pipe = Pipeline()
pipe.add_component("f1_tool", f1_tool)
pipe.add_component("builder", builder)
pipe.add_component("llm", llm)

# Connect the pipeline components
pipe.connect("f1_tool.service_response", "builder.service_response")
pipe.connect("builder.prompt", "llm.messages")

# Run the pipeline with a question
def query_f1_pipeline(user_query: str):
    """
    Run the F1 bot pipeline with the user's query.
    :param user_query: The user's query as a string.
    :return: The response generated by the LLM.
    """
    result = pipe.run(data={
        "f1_tool": {
            "messages": [ChatMessage.from_user(user_query)]
        },
        "builder": {
            "user_message": ChatMessage.from_user("Answer the F1 query in a user-friendly way")
        }
    })
    return result["llm"]["replies"][0].content

# Main function for CLI
def main():
    parser = argparse.ArgumentParser(description="Query the F1 pipeline CLI tool.")
    parser.add_argument("query", type=str, help="User query. E.g: 'Who won in Monaco in 2024?'")
    parser.add_argument("--model", type=str, default="gpt-4", help="Model name")
    parser.add_argument("--timeout", type=int, default=10, help="Timeout in seconds")
    parser.add_argument("--max_tokens", type=int, default=1024, help="Maximum tokens for response")

    args = parser.parse_args()

    # Update LLM configuration with arguments
    llm.generation_kwargs["max_tokens"] = args.max_tokens

    # Process the query
    response = query_f1_pipeline(args.query)
    print("LLM Response:", response)

if __name__ == "__main__":
    main()
