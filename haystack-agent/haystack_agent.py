import os
import time
import argparse
from haystack import Pipeline
from haystack.dataclasses import ChatMessage
from haystack.components.builders import ChatPromptBuilder
from haystack.components.generators.chat import OpenAIChatGenerator
from haystack_experimental.components.tools.openapi import OpenAPITool, LLMProvider

from config.env import OPENAI_API_KEY

# # Ensure required environment variables are set
# if "OPENAI_API_KEY" not in os.environ:
#     raise ValueError("OPENAI_API_KEY environment variable not set")

# Constants
ALLOW_DANGEROUS_REQUEST = True

print("initializing OpenAPI tool f1_tool ...")

tic = time.perf_counter()

# Initialize the OpenAPI tool with the F1 API schema
f1_tool = OpenAPITool(
    generator_api=LLMProvider.OPENAI,
    spec="openapi.yaml",
)

toc = time.perf_counter()
print("===============================================================")
print(f"=== DONE: invoke the agent - {toc - tic:0.4f} seconds ==========")
print("===============================================================")

print("OpenAPI tool f1_tool initialized")
#print(f"Tool functions: {f1_tool.options['functions']}")
#print(f"Tool description: {f1_tool}")

# Define the prompt
messages = [
    ChatMessage.from_system("Answer the F1 query using the API. Race names with two words should be separated by an underscore and be in lowercase. The API stores data from 2021 to 2024."),
    ChatMessage.from_user("User asked: {{user_message}}"),
    ChatMessage.from_system("API responded: {{service_response}}")
]

builder = ChatPromptBuilder(template=messages)

# Initialize the LLM
llm = OpenAIChatGenerator(generation_kwargs={"max_tokens": 1024})

print("LLM and tools initialized.")

# Define the pipeline components
pipe = Pipeline()
pipe.add_component("f1_tool", f1_tool)
pipe.add_component("builder", builder)
pipe.add_component("llm", llm)

# Connect the pipeline components
pipe.connect("f1_tool.service_response", "builder.service_response")
pipe.connect("builder.prompt", "llm.messages")

print("Pipeline constructed.")

# Run the pipeline with a question
def query_f1_pipeline(user_query: str):
    """
    Run the F1 bot pipeline with the user's query.
    :param user_query: The user's query as a string.
    :return: The response generated by the LLM.
    """
    print(f"executing Pipeline with query: [{user_query}] ...")

    tic = time.perf_counter()

    result = pipe.run(data={
        "f1_tool": {
            "messages": [ChatMessage.from_user(user_query)]
        },
        "builder": {
            "user_message": ChatMessage.from_user("Answer the F1 query in a user-friendly way")
        }
    })

    toc = time.perf_counter()
    print("===============================================================")
    print(f"=== DONE: invoke the agent - {toc - tic:0.4f} seconds ==========")
    print("===============================================================")

#    print("Pipeline execution completed.")

    return result["llm"]["replies"][0].content

# Main function for CLI
def main():
    parser = argparse.ArgumentParser(description="Query the F1 pipeline CLI tool.")
    parser.add_argument("query", type=str, help="User query. E.g: 'Who won in Monaco in 2024?'")
    parser.add_argument("--model", type=str, default="gpt-4o-mini", help="Model name")
    parser.add_argument("--timeout", type=int, default=10, help="Timeout in seconds")
    parser.add_argument("--max_tokens", type=int, default=1024, help="Maximum tokens for response")

    args = parser.parse_args()

    # Update LLM configuration with arguments
    llm.generation_kwargs["max_tokens"] = args.max_tokens

    #print(f"Using model: {args.model} with max tokens: {args.max_tokens}")
    print(f"Processing query: {args.query}")

    # Process the query
    response = query_f1_pipeline(args.query)
    print("LLM Response:", response)

if __name__ == "__main__":
    main()
